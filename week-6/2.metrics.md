---
theme: gaia
_class: lead
paginate: true
backgroundColor: #fff
backgroundImage: url('https://marp.app/assets/hero-background.svg')
marp: true
---

# Metric

---

# Metric

- 로그가 이벤트성 데이터라면
- 메트릭은 어느 시점에 현재 상태를 표시하는 지표

---

# Metric

- 로그는 이미 발생한 일을 수습하는 것이 목표라면
- 메트릭은 이것에 더해서 문제가 터지기 전에 예방하는 목적이 강함
- 메트릭을 시각화할 때는 이상 조건이 무엇인지를 항상 명심해야 함.
  e.g.,
  - conntrack 이 30만이니깐 TCP 커넥션 개수가 30만을 넘기지 않는지 모니터링 (임계값 존재)
  - CPU 사용량이 평소에는 50% 수준인데 갑자기 90%를 찍지는 않는지 모니터링 (평소와 다른 패턴)

---

# Metrics

여러 프로젝트들이 존재하지만 Prometheus 가 제일 기본
Prometheus로 시작해서 부족하면 다른 것으로 옮겨가는 것이 좋음

1. 고가용성이 필요 > [link](https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage)
2. 운영부담 줄이기 > Datadog, Elasticsearch

---

# Metric

보통은 Prometheus 를 기반으로 만들어진 OpenMetrics 포맷에 맞춰서 특정 http 기반 endpoint 를 통해서 수집
https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-based-format

```
metric_name [
  "{" label_name "=" `"` label_value `"` { "," label_name "=" `"` label_value `"` } [ "," ] "}"
] value [ timestamp ]
```

http://127.0.0.1:8001/metrics

---

# Metric

흔히 다음으로부터 메트릭을 가져옴

1. kubelet
2. kube-apiserver
3. kube-state-metrics
4. node-exporter
5. etcd

---

다만 처음부터 모든 메트릭을 전부 보려고 노력하는 것은 리소스 낭비
어떤 게 존재하는지만 알아두었다가 실제 문제가 발생하는 것만 시각화하는 것이 좋음

다음은 node-exporter에서 추출되는 메트릭 중 볼만한 것들

---

# DEMO

메트릭 리뷰하는 법

---

# Metrics - 지속적으로 봐줘야 하는 것

1. CPU
2. Memory
3. Network
4. Disk
5. API

---

# Metrics - CPU

CPU로 보통 세 가지 이슈를 파악 가능

1. 애플리케이션 버그로 Spike 치는 경우
   노드 단위의 모니터링 필요

   - 단기간이라도 초 단위 메트릭 확보 필요
   - 서비스 응답속도가 빨라졌다 느려졌다 한다면 의심 가능

---

# Metrics - CPU

CPU로 보통 세 가지 이슈를 파악 가능

2. 더 높은 CPU 성능이 필요한 경우
   > load 1m 5m 15m 등의 기본적인 지표 확인 필요
   > Pod / Workload / Node 단위로 확인 필요
   > 장기적인 추세확인이 필요

---

# Metrics - CPU

CPU로 보통 세 가지 이슈를 파악 가능

3. CPU 가 아닌것의 이슈 [link](https://scoutapm.com/blog/understanding-linuxs-cpu-stats)
   - (D) User : Pod 에서 사용하는 CPU
   - (A) System : IPVS, eBPF 등에서 사용하는 CPU. 이게 높다면 K8s 구조개선이 필요
   - (A) Niced : 0.0 이 이상적. Realtime Application 에서 사용
   - (D) Idle : 높을수록 문제는 적어지지만 비용낭비가 심함
   - (I) Wait : Disk, Network IO 등에 문제가 있을 가능성이 높음
   - (I) HW / SW interrupt : 이게 문제가 있다면 커널에 버그가 있는것
   - (I) Steal : VM 을 빡빡하게 사용하고 있다는 지표. % 보다는 이 값이 CPU 부하와 같이 움직이는지 여부가 중요.

---

# Metrics - Memory

실제 메모리에 여유가 있는지 파악하는것은 상당히 어려운 작업

[Go 언어](https://github.com/golang/go/issues/42330) 같은 경우 얼마 전까지만 해도 일단 메모리 점유하고 있다가 [Kernel 쪽에서 메모리 부족](https://man7.org/linux/man-pages/man2/madvise.2.html)하다고 하면 그제야 돌려주는 게 기본 정책.

JVM, Ruby 쪽에도 비슷한 이슈 존재

---

# Metrics - Memory

가장 좋은방법은

0. Kubernetes에서 관리하지 않는 프로그램을 최소화한 뒤 (apt 조차 문제가 될 수 있음)
1. [System Reserved](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#kube-reserved) 를 잘 설정해두고
2. Memory Requests 와 Limits 언제나 Guaranteed 로 설정하는것

개별 Pod 은 좀 더 자주 죽겠지만, 이런 조치를 취하고도 노드가 죽는 경우는 본 적 없음

---

# Metrics - Memory

다만 문제는 적정 값이 어느 정도인지 알기가 힘들다는 것

개인적으로 권장은

1. Requests / Limits 도 없이 돌리다가 ([BestEffort](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort)) (문제 발생하면 이 Pod 이 가장 먼저 제거됨)
2. 일주일 정도 후에 피크 값의 두 배 정도 되는 값을 limits로 설정하는 것

(이것을 자동으로 해주는 컨트롤러를 만들어볼까도 생각중)

---

# Metrics - Network

네트워크는 시에 문제가 발생하면 메트릭으로 못 볼 가능성이 높기 때문에 오히려 단순(?)
봐야할 지표 추천

```bash
node_netstat_TcpExt_TCPSynRetrans # TCP Retransmission 횟수. 갑자기 피크가 친다면 네트워크 장비 이슈일 가능성이 큼
node_netstat_Tcp_\(ActiveOpens|node_netstat_Tcp_CurrEstab) # 현재 연결된 TCP Connection 수. nf_conntrack limit 넘어가지 않도록 모니터링 필요
node_network_\(receive|transmit)\_(packets|bytes|drop|errs)_total # bandwidth 전부 사용했는지 감시용
```

---

# Metrics - DISK

응답속도 확인에 초점

```bash
node_disk_read_bytes_total
node_disk_read_time_seconds_total
node_disk_write_bytes_total
node_disk_wrtie_time_seconds_total
```

더 깊이 들어간다면 다음 글 추천 [글](https://devconnected.com/monitoring-disk-i-o-on-linux-with-the-node-exporter/)

---

# Metrics - API

`apiserver_request_total` 의 변화량이 초당 200을 넘어가지 않도록.
나머지는 메모리 모니터링만 잘하면 충분.
